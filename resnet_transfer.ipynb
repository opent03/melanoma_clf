{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import torchvision.models as torchmodels\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_train_test(datadir, valid_size=.2, batch_size=32, num_workers=6, size=(200,200)):\n",
    "    'Load train/test data'\n",
    "    normalize = torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    train_transforms = transforms.Compose([transforms.Resize(size),\n",
    "                                           transforms.ToTensor(),\n",
    "                                           normalize,\n",
    "                                       ])\n",
    "    test_transforms = transforms.Compose([transforms.Resize(size),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          normalize,\n",
    "                                      ])\n",
    "    train_data = datasets.ImageFolder(datadir,\n",
    "                    transform=train_transforms)\n",
    "    test_data = datasets.ImageFolder(datadir,\n",
    "                    transform=test_transforms)\n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    from torch.utils.data.sampler import SubsetRandomSampler\n",
    "    train_idx, test_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "    train = torch.utils.data.DataLoader(train_data,\n",
    "                   sampler=train_sampler, batch_size=batch_size, \n",
    "                                        num_workers=num_workers, pin_memory=True)\n",
    "    test = torch.utils.data.DataLoader(test_data,\n",
    "                   sampler=test_sampler, batch_size=batch_size, \n",
    "                                       num_workers=num_workers, pin_memory=True)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup variables\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "learning_rate = 1e-3\n",
    "imgsize = (128, 128)\n",
    "transfer = False\n",
    "root = 'data/'\n",
    "train_loader, test_loader = load_split_train_test(root, batch_size=batch_size, size=imgsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What an amazing world we live in...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model\n",
    "net = torchmodels.resnet50(pretrained=False)\n",
    "ct = 0\n",
    "if transfer:\n",
    "    for child in net.children():\n",
    "        ct += 1\n",
    "        if ct < 7:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "net.fc = nn.Linear(2048, 8)\n",
    "# Other model-related shtuff\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "exp_scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"What an amazing world we live in...\")\n",
    "    net = net.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "# Init weights\n",
    "def init_weights(m):\n",
    "        if type(m) == nn.Conv2d:\n",
    "            torch.nn.init.kaiming_normal_(m.weight)\n",
    "net.apply(init_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_loader, optimizer, scheduler, criterion, epoch, batch_size):\n",
    "    'Train loop'\n",
    "    net.train()\n",
    "    scheduler.step()\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, target = Variable(images), Variable(labels)\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "            target = target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = net(images)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batch_idx + 1) % 20 == 0:\n",
    "            batch_size = len(images)\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, (batch_idx + 1) * len(images), len(train_loader) * batch_size,\n",
    "                100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, batch_size):\n",
    "    'Evaluate loop'\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for _, (data, target) in enumerate(data_loader):\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            if torch.cuda.is_available():\n",
    "                data = data.cuda()\n",
    "                target = target.cuda()\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            loss += F.cross_entropy(output, target, size_average=False).data.item()\n",
    "\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "\n",
    "        loss /= len(data_loader.dataset)\n",
    "        acc = float(100. * correct) / float(len(data_loader)*batch_size)\n",
    "        print('Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)'.format(\n",
    "            loss, correct, len(data_loader)*batch_size, acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [2560/20352 (13%)]\tLoss: 1.270259\n",
      "Train Epoch: 1 [5120/20352 (25%)]\tLoss: 1.370888\n",
      "Train Epoch: 1 [7680/20352 (38%)]\tLoss: 2.062752\n",
      "Train Epoch: 1 [10240/20352 (50%)]\tLoss: 1.251606\n",
      "Train Epoch: 1 [12800/20352 (63%)]\tLoss: 1.210032\n",
      "Train Epoch: 1 [15360/20352 (75%)]\tLoss: 1.221559\n",
      "Train Epoch: 1 [17920/20352 (88%)]\tLoss: 1.052256\n",
      "test_accuracy: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teiv/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 0.9496, Accuracy: 2553/5120 (49.863%)\n",
      "Train Epoch: 2 [2560/20352 (13%)]\tLoss: 1.131922\n",
      "Train Epoch: 2 [5120/20352 (25%)]\tLoss: 1.052589\n",
      "Train Epoch: 2 [7680/20352 (38%)]\tLoss: 0.984476\n",
      "Train Epoch: 2 [10240/20352 (50%)]\tLoss: 1.042102\n",
      "Train Epoch: 2 [12800/20352 (63%)]\tLoss: 1.122846\n",
      "Train Epoch: 2 [15360/20352 (75%)]\tLoss: 1.180307\n",
      "Train Epoch: 2 [17920/20352 (88%)]\tLoss: 1.167070\n",
      "test_accuracy: \n",
      "Average loss: 0.2320, Accuracy: 2929/5120 (57.207%)\n",
      "Train Epoch: 3 [2560/20352 (13%)]\tLoss: 1.047372\n",
      "Train Epoch: 3 [5120/20352 (25%)]\tLoss: 1.083418\n",
      "Train Epoch: 3 [7680/20352 (38%)]\tLoss: 1.251954\n",
      "Train Epoch: 3 [10240/20352 (50%)]\tLoss: 1.065370\n",
      "Train Epoch: 3 [12800/20352 (63%)]\tLoss: 1.022250\n",
      "Train Epoch: 3 [15360/20352 (75%)]\tLoss: 1.168136\n",
      "Train Epoch: 3 [17920/20352 (88%)]\tLoss: 1.141750\n",
      "test_accuracy: \n",
      "Average loss: 0.2199, Accuracy: 3065/5120 (59.863%)\n",
      "Train Epoch: 4 [2560/20352 (13%)]\tLoss: 0.967916\n",
      "Train Epoch: 4 [5120/20352 (25%)]\tLoss: 0.914249\n",
      "Train Epoch: 4 [7680/20352 (38%)]\tLoss: 0.978853\n",
      "Train Epoch: 4 [10240/20352 (50%)]\tLoss: 0.914316\n",
      "Train Epoch: 4 [12800/20352 (63%)]\tLoss: 1.059394\n",
      "Train Epoch: 4 [15360/20352 (75%)]\tLoss: 1.055849\n",
      "Train Epoch: 4 [17920/20352 (88%)]\tLoss: 0.977370\n",
      "test_accuracy: \n",
      "Average loss: 0.2117, Accuracy: 3149/5120 (61.504%)\n",
      "Train Epoch: 5 [2560/20352 (13%)]\tLoss: 0.948825\n",
      "Train Epoch: 5 [5120/20352 (25%)]\tLoss: 1.043222\n",
      "Train Epoch: 5 [7680/20352 (38%)]\tLoss: 1.014874\n",
      "Train Epoch: 5 [10240/20352 (50%)]\tLoss: 0.948312\n",
      "Train Epoch: 5 [12800/20352 (63%)]\tLoss: 0.901396\n",
      "Train Epoch: 5 [15360/20352 (75%)]\tLoss: 0.932730\n",
      "Train Epoch: 5 [17920/20352 (88%)]\tLoss: 1.103235\n",
      "test_accuracy: \n",
      "Average loss: 0.2095, Accuracy: 3120/5120 (60.938%)\n",
      "Train Epoch: 6 [2560/20352 (13%)]\tLoss: 0.884908\n",
      "Train Epoch: 6 [5120/20352 (25%)]\tLoss: 0.736677\n",
      "Train Epoch: 6 [7680/20352 (38%)]\tLoss: 0.895638\n",
      "Train Epoch: 6 [10240/20352 (50%)]\tLoss: 0.898961\n",
      "Train Epoch: 6 [12800/20352 (63%)]\tLoss: 0.882398\n",
      "Train Epoch: 6 [15360/20352 (75%)]\tLoss: 0.824282\n",
      "Train Epoch: 6 [17920/20352 (88%)]\tLoss: 0.823375\n",
      "test_accuracy: \n",
      "Average loss: 0.2056, Accuracy: 3201/5120 (62.520%)\n",
      "Train Epoch: 7 [2560/20352 (13%)]\tLoss: 0.658716\n",
      "Train Epoch: 7 [5120/20352 (25%)]\tLoss: 0.791411\n",
      "Train Epoch: 7 [7680/20352 (38%)]\tLoss: 0.734773\n",
      "Train Epoch: 7 [10240/20352 (50%)]\tLoss: 0.750216\n",
      "Train Epoch: 7 [12800/20352 (63%)]\tLoss: 0.741516\n",
      "Train Epoch: 7 [15360/20352 (75%)]\tLoss: 0.797153\n",
      "Train Epoch: 7 [17920/20352 (88%)]\tLoss: 0.878435\n",
      "test_accuracy: \n",
      "Average loss: 0.2088, Accuracy: 3233/5120 (63.145%)\n",
      "Train Epoch: 8 [2560/20352 (13%)]\tLoss: 0.583488\n",
      "Train Epoch: 8 [5120/20352 (25%)]\tLoss: 0.698364\n",
      "Train Epoch: 8 [7680/20352 (38%)]\tLoss: 0.703858\n",
      "Train Epoch: 8 [10240/20352 (50%)]\tLoss: 0.575560\n",
      "Train Epoch: 8 [12800/20352 (63%)]\tLoss: 0.689949\n",
      "Train Epoch: 8 [15360/20352 (75%)]\tLoss: 0.859103\n",
      "Train Epoch: 8 [17920/20352 (88%)]\tLoss: 0.831849\n",
      "test_accuracy: \n",
      "Average loss: 0.2093, Accuracy: 3125/5120 (61.035%)\n",
      "Train Epoch: 9 [2560/20352 (13%)]\tLoss: 0.559864\n",
      "Train Epoch: 9 [5120/20352 (25%)]\tLoss: 0.541700\n",
      "Train Epoch: 9 [7680/20352 (38%)]\tLoss: 0.454698\n",
      "Train Epoch: 9 [10240/20352 (50%)]\tLoss: 0.423680\n",
      "Train Epoch: 9 [12800/20352 (63%)]\tLoss: 0.548869\n",
      "Train Epoch: 9 [15360/20352 (75%)]\tLoss: 0.368593\n",
      "Train Epoch: 9 [17920/20352 (88%)]\tLoss: 0.530810\n",
      "test_accuracy: \n",
      "Average loss: 0.2343, Accuracy: 3234/5120 (63.164%)\n",
      "Train Epoch: 10 [2560/20352 (13%)]\tLoss: 0.277848\n",
      "Train Epoch: 10 [5120/20352 (25%)]\tLoss: 0.312513\n",
      "Train Epoch: 10 [7680/20352 (38%)]\tLoss: 0.241779\n",
      "Train Epoch: 10 [10240/20352 (50%)]\tLoss: 0.309233\n",
      "Train Epoch: 10 [12800/20352 (63%)]\tLoss: 0.453808\n",
      "Train Epoch: 10 [15360/20352 (75%)]\tLoss: 0.421064\n",
      "Train Epoch: 10 [17920/20352 (88%)]\tLoss: 0.366153\n",
      "test_accuracy: \n",
      "Average loss: 0.2656, Accuracy: 3143/5120 (61.387%)\n"
     ]
    }
   ],
   "source": [
    "# Training step happens here\n",
    "def lazy():\n",
    "    for epoch in range(epochs):\n",
    "        epoch += 1\n",
    "        train(net, train_loader, optimizer, exp_scheduler, criterion, epoch, batch_size)\n",
    "        #print('train accuracy: ')\n",
    "        #tracc = evaluate(net, train_loader, batch_size)\n",
    "        print('test_accuracy: ')\n",
    "        teacc = evaluate(net, test_loader, batch_size)\n",
    "lazy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model first iteration\n",
    "torch.save(net.state_dict, './resnet50-round1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower learning rate\n",
    "learning_rate = 4e-4\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "exp_scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [2560/20352 (13%)]\tLoss: 0.641072\n",
      "Train Epoch: 1 [5120/20352 (25%)]\tLoss: 0.719089\n",
      "Train Epoch: 1 [7680/20352 (38%)]\tLoss: 0.503528\n",
      "Train Epoch: 1 [10240/20352 (50%)]\tLoss: 0.719566\n",
      "Train Epoch: 1 [12800/20352 (63%)]\tLoss: 0.613006\n",
      "Train Epoch: 1 [15360/20352 (75%)]\tLoss: 0.594290\n",
      "Train Epoch: 1 [17920/20352 (88%)]\tLoss: 0.806315\n",
      "test_accuracy: \n",
      "Average loss: 0.2413, Accuracy: 3118/5120 (60.898%)\n",
      "Train Epoch: 2 [2560/20352 (13%)]\tLoss: 0.627106\n",
      "Train Epoch: 2 [5120/20352 (25%)]\tLoss: 0.606147\n",
      "Train Epoch: 2 [7680/20352 (38%)]\tLoss: 0.435087\n",
      "Train Epoch: 2 [10240/20352 (50%)]\tLoss: 0.508033\n",
      "Train Epoch: 2 [12800/20352 (63%)]\tLoss: 0.521972\n",
      "Train Epoch: 2 [15360/20352 (75%)]\tLoss: 0.451593\n",
      "Train Epoch: 2 [17920/20352 (88%)]\tLoss: 0.543754\n",
      "test_accuracy: \n",
      "Average loss: 0.2380, Accuracy: 3220/5120 (62.891%)\n",
      "Train Epoch: 3 [2560/20352 (13%)]\tLoss: 0.281652\n",
      "Train Epoch: 3 [5120/20352 (25%)]\tLoss: 0.271877\n",
      "Train Epoch: 3 [7680/20352 (38%)]\tLoss: 0.215133\n",
      "Train Epoch: 3 [10240/20352 (50%)]\tLoss: 0.335313\n",
      "Train Epoch: 3 [12800/20352 (63%)]\tLoss: 0.266869\n",
      "Train Epoch: 3 [15360/20352 (75%)]\tLoss: 0.307341\n",
      "Train Epoch: 3 [17920/20352 (88%)]\tLoss: 0.237166\n",
      "test_accuracy: \n",
      "Average loss: 0.2879, Accuracy: 3255/5120 (63.574%)\n",
      "Train Epoch: 4 [2560/20352 (13%)]\tLoss: 0.221228\n",
      "Train Epoch: 4 [5120/20352 (25%)]\tLoss: 0.111124\n",
      "Train Epoch: 4 [7680/20352 (38%)]\tLoss: 0.195291\n",
      "Train Epoch: 4 [10240/20352 (50%)]\tLoss: 0.200223\n",
      "Train Epoch: 4 [12800/20352 (63%)]\tLoss: 0.156998\n",
      "Train Epoch: 4 [15360/20352 (75%)]\tLoss: 0.177649\n",
      "Train Epoch: 4 [17920/20352 (88%)]\tLoss: 0.176602\n",
      "test_accuracy: \n",
      "Average loss: 0.3126, Accuracy: 3202/5120 (62.539%)\n",
      "Train Epoch: 5 [2560/20352 (13%)]\tLoss: 0.149143\n",
      "Train Epoch: 5 [5120/20352 (25%)]\tLoss: 0.113595\n",
      "Train Epoch: 5 [7680/20352 (38%)]\tLoss: 0.147185\n",
      "Train Epoch: 5 [10240/20352 (50%)]\tLoss: 0.159417\n",
      "Train Epoch: 5 [12800/20352 (63%)]\tLoss: 0.125756\n",
      "Train Epoch: 5 [15360/20352 (75%)]\tLoss: 0.205807\n",
      "Train Epoch: 5 [17920/20352 (88%)]\tLoss: 0.177881\n",
      "test_accuracy: \n",
      "Average loss: 0.3405, Accuracy: 3160/5120 (61.719%)\n",
      "Train Epoch: 6 [2560/20352 (13%)]\tLoss: 0.055286\n",
      "Train Epoch: 6 [5120/20352 (25%)]\tLoss: 0.089778\n",
      "Train Epoch: 6 [7680/20352 (38%)]\tLoss: 0.067928\n",
      "Train Epoch: 6 [10240/20352 (50%)]\tLoss: 0.067792\n",
      "Train Epoch: 6 [12800/20352 (63%)]\tLoss: 0.038230\n",
      "Train Epoch: 6 [15360/20352 (75%)]\tLoss: 0.072493\n",
      "Train Epoch: 6 [17920/20352 (88%)]\tLoss: 0.048191\n",
      "test_accuracy: \n",
      "Average loss: 0.3915, Accuracy: 3268/5120 (63.828%)\n",
      "Train Epoch: 7 [2560/20352 (13%)]\tLoss: 0.013128\n",
      "Train Epoch: 7 [5120/20352 (25%)]\tLoss: 0.068170\n",
      "Train Epoch: 7 [7680/20352 (38%)]\tLoss: 0.024095\n",
      "Train Epoch: 7 [10240/20352 (50%)]\tLoss: 0.018462\n",
      "Train Epoch: 7 [12800/20352 (63%)]\tLoss: 0.014378\n",
      "Train Epoch: 7 [15360/20352 (75%)]\tLoss: 0.006988\n",
      "Train Epoch: 7 [17920/20352 (88%)]\tLoss: 0.006570\n",
      "test_accuracy: \n",
      "Average loss: 0.4345, Accuracy: 3261/5120 (63.691%)\n",
      "Train Epoch: 8 [2560/20352 (13%)]\tLoss: 0.045034\n",
      "Train Epoch: 8 [5120/20352 (25%)]\tLoss: 0.056957\n",
      "Train Epoch: 8 [7680/20352 (38%)]\tLoss: 0.015929\n",
      "Train Epoch: 8 [10240/20352 (50%)]\tLoss: 0.016876\n",
      "Train Epoch: 8 [12800/20352 (63%)]\tLoss: 0.004364\n",
      "Train Epoch: 8 [15360/20352 (75%)]\tLoss: 0.004709\n",
      "Train Epoch: 8 [17920/20352 (88%)]\tLoss: 0.120988\n",
      "test_accuracy: \n",
      "Average loss: 0.4465, Accuracy: 3297/5120 (64.395%)\n",
      "Train Epoch: 9 [2560/20352 (13%)]\tLoss: 0.025444\n",
      "Train Epoch: 9 [5120/20352 (25%)]\tLoss: 0.005852\n",
      "Train Epoch: 9 [7680/20352 (38%)]\tLoss: 0.016747\n",
      "Train Epoch: 9 [10240/20352 (50%)]\tLoss: 0.012737\n",
      "Train Epoch: 9 [12800/20352 (63%)]\tLoss: 0.002349\n",
      "Train Epoch: 9 [15360/20352 (75%)]\tLoss: 0.012494\n",
      "Train Epoch: 9 [17920/20352 (88%)]\tLoss: 0.039978\n",
      "test_accuracy: \n",
      "Average loss: 0.4439, Accuracy: 3293/5120 (64.316%)\n",
      "Train Epoch: 10 [2560/20352 (13%)]\tLoss: 0.002231\n",
      "Train Epoch: 10 [5120/20352 (25%)]\tLoss: 0.002244\n",
      "Train Epoch: 10 [7680/20352 (38%)]\tLoss: 0.001813\n",
      "Train Epoch: 10 [10240/20352 (50%)]\tLoss: 0.000766\n",
      "Train Epoch: 10 [12800/20352 (63%)]\tLoss: 0.001723\n",
      "Train Epoch: 10 [15360/20352 (75%)]\tLoss: 0.004863\n",
      "Train Epoch: 10 [17920/20352 (88%)]\tLoss: 0.007401\n",
      "test_accuracy: \n",
      "Average loss: 0.4831, Accuracy: 3323/5120 (64.902%)\n"
     ]
    }
   ],
   "source": [
    "lazy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
